I'm the abstract superclass for all benchmarks for the TraceDebugger. A benchmark runs a specific portion of code (optionally accompanied by some preparation and postparation logic) and measures the time required for the evaluation. My design is strongly inspired by SUnit's TestCase, including means to organize benchmarks in subclasses, debugging them, and the integration into Browser/CodeHolder.

DESIGN NOTES: Albeit overall functionality, this mini-framework is connotated negatively by the author for multiple reasons:
- As discovered later, many design goals of this framework have already been reached by the BenchmarkRunner <http://forum.world.st/A-Benchmarking-tool-for-the-trunk-td4892463.html>. Integration of our few specialties into this solution would be more economic.
- With the goal in mind to run these tests on the CI, our focus on macrobenchmarks has proven as a problem. Both runtimes of the benchmarks and the variance of their results tend to be very high, limiting the significance of the results. On the other hand, there is no mechanism yet to repeat single benchmarks easily for a given time (as BlockClosure>>#bench does).
- Another problem with running benchmarks on CI is the fluctuating performance of GitHub Actions runners. Right now, this could be only mitigated by introducing control measures for normalization (and hoping that the runner performance is constant within a single runner instance). Another chance would lie in measuring the user time of the VM only, but this is currently not supported by OSVM <http://lists.squeakfoundation.org/pipermail/squeak-dev/2022-January/218209.html>. A different approach would be to use SimulationStudio's LimitSimulator which abstracts from the hardware performance, but this is a very rough approximation to benchmarking only without a proper cost model and not applicable to macrobenchmark for performance reasons.

tl;dr: When further efforts need to be made to the TraceDebugger's benchmarking infrastructure, it would probably be the best solution to migrate to BenchmarkRunner and rewrite as many macrobenchmarks as possible to microbenchmarks.